{"cells":[{"cell_type":"markdown","source":["# Introduction to Spark programming"],"metadata":{}},{"cell_type":"markdown","source":["# Word Count\n\nThe \"Hello World!\" of distributed programming is the wordcount. Basically, you want to count easily number of different words contained in an unstructured text. You will write some code to perform this task on the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) retrieved from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page).\n\n[Spark's Python API reference](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) could provide some help\n\n### ** Part 1: Creating a base RDD and pair RDDs **\n\n#### In this part of the lab, we will explore creating a base RDD with `parallelize` and using pair RDDs to count words.\n\n#### We'll start by generating a base RDD by using a Python list and the `sc.parallelize` method.  Then we'll print out the type of the base RDD."],"metadata":{}},{"cell_type":"code","source":["# Please run this cell to load the Test class\nfrom test_helper import Test"],"metadata":{"collapsed":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":["words_list = ['we', 'few', 'we', 'happy', 'few', \"we\", \"band\", \"of\", \"brothers\"]\nwords_RDD = sc.parallelize(words_list, 4)\n\n# Print the type of words_RDD\nprint type(words_RDD)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["We want to capitalize each word contained in a RDD. For such transformation, we use a `map`, as we want to transform a RDD of **n** elements into another RDD of **n** using a function that gets and returns one single element.\n\nPlease implement `capitalize`function in the cell below."],"metadata":{}},{"cell_type":"code","source":["def capitalize(word):\n    \"\"\"Capitalize lowercase `words`.\n\n    Args:\n        word (str): A lowercase string.\n\n    Returns:\n        str: A string which first letter is uppercase.\n    \"\"\"\n    return word.capitalize()\n\nprint(capitalize('we'))\n\nTest.assertEquals(capitalize('we'), 'We', \"Capitalize\")"],"metadata":{"collapsed":false},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Apply `capitalize` to the base RDD, using a [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) transformation that applies the `capitalize()` function to each element. Then call the [collect()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) action to retrieve the values of the transformed RDD, and print them."],"metadata":{}},{"cell_type":"code","source":["capital_RDD = ???\nlocal_result = ???\nprint(local_result)\n\nTest.assertEqualsHashed(local_result, 'bd73c54004cc9655159aceb703bc14fe93369fb1',\n                        'incorrect value for local_data')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Do the same using a lambda function"],"metadata":{}},{"cell_type":"code","source":["capital_lambda_RDD = ???\nlocal_result = ???\nprint(local_result)\n\nTest.assertEqualsHashed(local_result, 'bd73c54004cc9655159aceb703bc14fe93369fb1',\n                        'incorrect value for capital_lambda_RDD')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Now use `map()` and a `lambda` function to return the number of characters in each word, and `collect` this result directly into a variable."],"metadata":{}},{"cell_type":"code","source":["plural_lengths = (capital_RDD.???.collect())\nprint(plural_lengths)\n\nTest.assertEqualsHashed(plural_lengths, '0772853c8e180c1bed8cfe9bde35aae79b277381',\n                  'incorrect values for plural_lengths')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["To program a wordcount, we will need `pair RDD` objects. A pair RDD is an RDD where each element is a pair tuple `(k, v)` where `k` is the key and `v` is the value. In this example, we will create a pair consisting of `('<word>', 1)` for each word element in the RDD.\n\nCreate the pair RDD using the `map()` transformation with a `lambda()` on `words_RDD`."],"metadata":{}},{"cell_type":"code","source":["words_pair_RDD = words_RDD.???\nprint(words_pair_RDD.collect())\n\nTest.assertEqualsHashed(words_pair_RDD.collect(), 'fb67a530034e01395386569ef29bf5565b503ec6',\n                        \"incorrect value for wrods_pair_RDD\")"],"metadata":{"collapsed":false},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Now, let's count the number of times a particular word appears in the RDD. There are multiple ways to perform the counting, but some are much less efficient or scalable than others.\n\nA naive approach would be to `collect()` all of the elements and count them in the driver program. While this approach could work for small datasets, it is not scalable as the result of `collect()` would have to fit in the driver's memory. When you should use `collect()` with care, always asking yourself what is the size of data you want to retrieve.\n\nIn order to program a scalable wordcount, you will need to use parallel operations.\n\n#### `groupByKey()` approach\n\nAn approach you might first consider is based on using the [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) transformation. This transformation groups all the elements of the RDD with the same key into a single list, stored in one of the partitions. \n \nUse `groupByKey()` on `words_pair_RDD`\n to generate a pair RDD of type `('word', list)`."],"metadata":{}},{"cell_type":"code","source":["words_grouped = words_pair_RDD.???\n\nfor key, value in words_grouped.collect():\n    print '{0}: {1}'.format(key, list(value))\n    \nTest.assertEqualsHashed(sorted(words_grouped.mapValues(lambda x: list(x)).collect()),\n                  'fdaad77fd81ef2df23d98ff7fd438fa700ca1fcf',\n                  'incorrect value for words_grouped')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Using the `groupByKey()` transformation results in an `pairRDD` containing words as keys, and Python iterators as values. Python iterators are a class of objects on which we can iterate, i.e.\n\n    a = some_iterator()\n    for elem in a:\n        # do stuff with elem\n\nPython lists and dictionnaries are iterators for example.\n\nNow sum the iterator using a `map()` transformation. The result should be a pair RDD consisting of (word, count) pairs.\n\nHint: there exists a `sum` function\nHint 2: you want to perform an operation only on the values of the pairRDD. Take a look at [mapValues()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.mapValues)."],"metadata":{}},{"cell_type":"code","source":["word_grouped_counts = words_grouped.???\nprint(word_grouped_counts.collect())\n\nTest.assertEqualsHashed(sorted(word_grouped_counts.collect()),\n                  'c20f05d36e98ae399b2cbe5b6cb9bf01b675455a',\n                  'incorrect value for word_grouped_counts')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["There are two problems with using `groupByKey()`:\n  + The operation requires a lot of data movement to move all the values into the appropriate partitions (remember the cost of network communications!).\n  + The lists can be very large. Consider a word count of English Wikipedia: the lists for common words (e.g., the, a, etc.) would be huge and could exhaust the available memory of a worker.\n\nA better approach is to start from the pair RDD and then use the [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) transformation to create a new pair RDD. The `reduceByKey()` transformation gathers together pairs that have the same key and applies the function provided to two values at a time, iteratively reducing all of the values to a single value. `reduceByKey()` operates by applying the function first within each partition on a per-key basis and then across the partitions, allowing it to scale efficiently to large datasets.\n\nCompute the word count using `reduceByKey`"],"metadata":{}},{"cell_type":"code","source":["word_counts = words_pair_RDD.???\nprint(word_counts.collect())\n\nTest.assertEqualsHashed(sorted(word_counts.collect()), 'c20f05d36e98ae399b2cbe5b6cb9bf01b675455a',\n                  'incorrect value for word_counts')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["You should be able to perform the word count by composing functions, resulting in a smaller code. Use the `map()` on word RDD to create a pair RDD, apply the `reduceByKey()` transformation, and `collect` in one statement."],"metadata":{}},{"cell_type":"code","source":["word_counts_collected = (words_RDD\n                         ???\n                         .collect()\n                        )\n\nprint(word_counts_collected)\n\nTest.assertEqualsHashed(sorted(word_counts_collected), 'c20f05d36e98ae399b2cbe5b6cb9bf01b675455a',\n                  'incorrect value for word_counts_collected')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Compute the number of unique words using one of the RDD you have already created."],"metadata":{}},{"cell_type":"code","source":["unique_words = ???\nprint(unique_words)\n\nTest.assertEquals(unique_words, 6, 'incorrect count of unique_words')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["Use a `reduce()` action to sum the counts in `wordCounts` and then divide by the number of unique words to find the mean number of words per unique word in `word_counts`.  First `map()` RDD `word_counts`, which consists of (key, value) pairs, to an RDD of values."],"metadata":{}},{"cell_type":"code","source":["from operator import add\ntotal_count = (word_counts\n              .map(???)\n              .reduce(???))\n\naverage = total_count / float(???)\nprint(total_count)\nprint(round(average, 2))\n\nTest.assertEquals(round(average, 2), 1.5, 'incorrect value of average')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["## Part 2: Apply word count to a file\n\nIn this section we will finish developing our word count application.  We'll have to build the `word_count` function, deal with real world problems like capitalization and punctuation, load in our data source, and compute the word count on the new data.\n\nFirst, define a function for word counting. You should reuse the techniques that have been covered in earlier parts of this lab.  This function should take in an RDD that is a list of words like `words_RDD` and return a pair RDD that has all of the words and their associated counts."],"metadata":{}},{"cell_type":"code","source":["def word_count(word_list_RDD):\n    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n\n    Args:\n        wordListRDD (RDD of str): An RDD consisting of words.\n\n    Returns:\n        RDD of (str, int): An RDD consisting of (word, count) tuples.\n    \"\"\"\n    ???\n\nprint(word_count(words_RDD).collect())\n\nTest.assertEqualsHashed(sorted(word_count(words_RDD).collect()),\n                      'c20f05d36e98ae399b2cbe5b6cb9bf01b675455a',\n                      'incorrect definition for word_count function')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Real world data is more complicated than the data we have been using in this lab. Some of the issues we have to address are:\n  + Words should be counted independent of their capitialization (e.g., Spark and spark should be counted as the same word).\n  + All punctuation should be removed.\n  + Any leading or trailing spaces on a line should be removed.\n \nDefine the function `removePunctuation` that converts all text to lower case, removes any punctuation, and removes leading and trailing spaces.  Use the Python [re](https://docs.python.org/2/library/re.html) module to remove any text that is not a letter, number, or space. Reading `help(re.sub)` might be useful.\n\nIf you have never used regex (regular expressions) before, you can refer to [Regular-expressions.info](http://www.regular-expressions.info/python.html)\n\nIn order to test your regular expressions, you can use [Regex Tester](http://www.regexpal.com)\n\nRegex can be a bit obscure at beginning, don't hesitate to search in [StackOverflow](http://stackoverflow.com) or to ask me for some help."],"metadata":{}},{"cell_type":"code","source":["import re\nimport string\n\n# Hint: string.punctuation contains all the punctuation symbols\n\ndef remove_punctuation(text):\n    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n\n    Note:\n        Only spaces, letters, and numbers should be retained.  Other characters should should be\n        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n        punctuation is removed.\n\n    Args:\n        text (str): A string.\n\n    Returns:\n        str: The cleaned up string.\n    \"\"\"\n    ???\n\nprint(remove_punctuation('Hello World!'))\nprint(remove_punctuation(' No under_score!'))\n\nTest.assertEquals(remove_punctuation(\"  Remove punctuation: there ARE trailing spaces. \"),\n                  'remove punctuation there are trailing spaces',\n                  'incorrect definition for remove_punctuation function')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["For the next part of this lab, we will use the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page). To convert a text file into an RDD, we use the `SparkContext.textFile()` method. We also apply the recently defined `remove_punctuation()` function using a `map()` transformation to strip out the punctuation and change all text to lowercase.  Since the file is large we use `take(15)`, instead of `collect()` so that we only print 15 lines.\n\nTake a look at [zipWithIndex()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.zipWithIndex) and [take()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) to understand the print statement"],"metadata":{}},{"cell_type":"code","source":["file_path = '/FileStore/tables/shakespeare.txt'\n\nshakespeare_RDD = (sc.textFile(file_path, 8)\n                     .map(remove_punctuation))\n\nprint('\\n'.join(shakespeare_RDD\n                .zipWithIndex()  # to (line, lineNum) pairRDD\n                .map(lambda (l, num): '{0}: {1}'.format(num, l))  # to 'lineNum: line'\n                .take(15)))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["Before we can use the `word_count()` function, we have to address two issues with the format of the RDD:\n  + #### The first issue is that  that we need to split each line by its spaces.\n  + #### The second issue is we need to filter out empty lines.\n \nApply a transformation that will split each element of the RDD by its spaces. For each element of the RDD, you should apply Python's string [split()](https://docs.python.org/2/library/string.html#string.split) function. You might think that a [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) transformation is the way to do this, but think about what the result of the `split()` function will be: there is a better option.\n\nHint: remember the problem we had with `GroupByKey()`"],"metadata":{}},{"cell_type":"code","source":["shakespeare_words_RDD = shakespeare_RDD.???\nshakespeare_word_count_elem = shakespeare_words_RDD.count()\nprint shakespeare_words_RDD.top(5)\nprint shakespeare_word_count_elem\n\n# This test allows for leading spaces to be removed either before or after\n# punctuation is removed.\nTest.assertTrue(shakespeare_word_count_elem == 927631 or shakespeare_word_count_elem == 928908,\n                'incorrect value for shakespeare_word_count_elem')\n\nTest.assertEqualsHashed(shakespeare_words_RDD.top(5),\n                  'f177c26ee0bc3a48368d7a92c08dd754237c3558',\n                  'incorrect value for shakespeare_words_RDD')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["The next step is to filter out the empty elements.  Remove all entries where the word is `''`."],"metadata":{}},{"cell_type":"code","source":["shakespeare_nonempty_words_RDD = shakespeare_words_RDD.???\nshakespeare_nonempty_word_elem_count = shakespeare_nonempty_words_RDD.count()\nprint(shakespeare_nonempty_word_elem_count)\n\nTest.assertEquals(shakespeare_nonempty_word_elem_count, 882996, \n                  'incorrect value for shakespeare_nonempty_word_elem_count')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["You now have an RDD that contains only words.  Next, apply the `word_count()` function to produce a list of word counts. We can view the top 15 words by using the `takeOrdered()` action. However, since the elements of the RDD are pairs, you will need a custom sort function that sorts using the value part of the pair.\n\nUse the `wordCount()` function and `takeOrdered()` to obtain the fifteen most common words and their counts."],"metadata":{}},{"cell_type":"code","source":["top15_words = ???\nprint '\\n'.join(map(lambda (w, c): '{0}: {1}'.format(w, c), top15_words))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":38},{"cell_type":"code","source":["Test.assertEqualsHashed(top15_words,\n                  'fbb4c8c74f98eeef70d021893f276231dfff55cb',\n                  'incorrect value for top15WordsAndCounts')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["You will notice that many of the words are common English words. These are called stopwords. In practice, when we do natural language processing, we filter these stopwords as they do not contain a lot of information."],"metadata":{"collapsed":true}}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.9","nbconvert_exporter":"python","file_extension":".py"},"name":"2-spark-rdd","notebookId":2466858938364087},"nbformat":4,"nbformat_minor":0}
